{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **PLEASE UPVOTE THE KERNEL IF YOU LIKE IT !** âœŒ","metadata":{}},{"cell_type":"markdown","source":"# **Lyrics Generation using RNN**\n### (can be generalised as coherent / meaningful text generation)","metadata":{"id":"TdrS4X7Z9Qw0"}},{"cell_type":"markdown","source":"#### **Importing TensorFlow and other libraries**","metadata":{"id":"nTaAxneu_OzW"}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport numpy as np\nimport os\nimport time","metadata":{"id":"Vp88IY6N9UJH","execution":{"iopub.status.busy":"2021-12-13T18:46:57.951554Z","iopub.execute_input":"2021-12-13T18:46:57.951988Z","iopub.status.idle":"2021-12-13T18:47:02.305196Z","shell.execute_reply.started":"2021-12-13T18:46:57.951952Z","shell.execute_reply":"2021-12-13T18:47:02.304442Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### **Specifying the path of the text file to process**","metadata":{"id":"9m_2sMc4_owO"}},{"cell_type":"code","source":"path_to_file = '../input/quality/quoters.txt'","metadata":{"id":"LZYe710ju3tz","execution":{"iopub.status.busy":"2021-12-13T18:47:02.307859Z","iopub.execute_input":"2021-12-13T18:47:02.308483Z","iopub.status.idle":"2021-12-13T18:47:02.312543Z","shell.execute_reply.started":"2021-12-13T18:47:02.308442Z","shell.execute_reply":"2021-12-13T18:47:02.311529Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### **Opening the text file in read mode**","metadata":{"id":"MO-5lBnNAFTE"}},{"cell_type":"code","source":"# Opening the text file in read mode and standard encoding it\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n\n# Length of text is the number of characters in it\nprint ('Length of text: {} characters'.format(len(text)))","metadata":{"id":"n1k2DRXZvG9d","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-13T18:47:02.314011Z","iopub.execute_input":"2021-12-13T18:47:02.314600Z","iopub.status.idle":"2021-12-13T18:47:03.424512Z","shell.execute_reply.started":"2021-12-13T18:47:02.314560Z","shell.execute_reply":"2021-12-13T18:47:03.423624Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# A look at the first 250 characters in text\nprint(text[:100])","metadata":{"id":"fHajuqtVvaOO","execution":{"iopub.status.busy":"2021-12-13T18:47:03.425974Z","iopub.execute_input":"2021-12-13T18:47:03.426606Z","iopub.status.idle":"2021-12-13T18:47:03.431635Z","shell.execute_reply.started":"2021-12-13T18:47:03.426564Z","shell.execute_reply":"2021-12-13T18:47:03.430753Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl._create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl._create_default_https_context = _create_unverified_https_context\n\ntokens = word_tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:38:37.973564Z","iopub.execute_input":"2021-12-13T20:38:37.973905Z","iopub.status.idle":"2021-12-13T20:39:55.005093Z","shell.execute_reply.started":"2021-12-13T20:38:37.973874Z","shell.execute_reply":"2021-12-13T20:39:55.004166Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"tokens_filtered = [word for word in tokens if word.isalnum()]","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:39:55.007079Z","iopub.execute_input":"2021-12-13T20:39:55.007445Z","iopub.status.idle":"2021-12-13T20:39:56.026057Z","shell.execute_reply.started":"2021-12-13T20:39:55.007408Z","shell.execute_reply":"2021-12-13T20:39:56.025100Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from nltk.probability import FreqDist\ndist = FreqDist(tokens_filtered)\ntokens_filtered_top = [pair[0] for pair in dist.most_common(20-1)]\ntokens_filtered_top","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:41:23.640028Z","iopub.execute_input":"2021-12-13T20:41:23.640393Z","iopub.status.idle":"2021-12-13T20:41:31.781213Z","shell.execute_reply.started":"2021-12-13T20:41:23.640362Z","shell.execute_reply":"2021-12-13T20:41:31.780096Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"vocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","metadata":{"id":"AExcxzvKvd1O","execution":{"iopub.status.busy":"2021-12-13T20:37:59.592977Z","iopub.execute_input":"2021-12-13T20:37:59.593312Z","iopub.status.idle":"2021-12-13T20:37:59.600030Z","shell.execute_reply.started":"2021-12-13T20:37:59.593281Z","shell.execute_reply":"2021-12-13T20:37:59.599151Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"#### **Preprocessing of text i.e from strings to numerical representation**\n##### Creating lookup-tables for char->num and num->char","metadata":{"id":"kDkpbEEmA2sx"}},{"cell_type":"code","source":"# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","metadata":{"id":"xpFuAN8XvlRg","execution":{"iopub.status.busy":"2021-12-13T18:47:03.997058Z","iopub.execute_input":"2021-12-13T18:47:03.997353Z","iopub.status.idle":"2021-12-13T18:47:16.119658Z","shell.execute_reply.started":"2021-12-13T18:47:03.997325Z","shell.execute_reply":"2021-12-13T18:47:16.118870Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print('{ ===========>')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n==========>}')","metadata":{"id":"y22qwVJlwOvV","execution":{"iopub.status.busy":"2021-12-13T18:47:16.121099Z","iopub.execute_input":"2021-12-13T18:47:16.121482Z","iopub.status.idle":"2021-12-13T18:47:16.133217Z","shell.execute_reply.started":"2021-12-13T18:47:16.121444Z","shell.execute_reply":"2021-12-13T18:47:16.132068Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Show how the first 20 characters from the text are mapped to integers\nprint ('{} ==> characters mapped to int ==> {}'.format(repr(text[:20]), text_as_int[:20]))","metadata":{"id":"P5-xHoW6wSgf","execution":{"iopub.status.busy":"2021-12-13T18:47:16.134693Z","iopub.execute_input":"2021-12-13T18:47:16.135024Z","iopub.status.idle":"2021-12-13T18:47:16.143122Z","shell.execute_reply.started":"2021-12-13T18:47:16.134991Z","shell.execute_reply":"2021-12-13T18:47:16.142070Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### **Creating training examples and targets**\nBroke the text into chunks of seq_length + 1 , if text is => \"Alright\" \n\nThen input sequence becomes => \"Alrigh\"\n\nThe output sequence becomes => \"lright\"","metadata":{"id":"dvv5ug90CPPg"}},{"cell_type":"code","source":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n  print(idx2char[i.numpy()] , end = \"\")","metadata":{"id":"PCl-BN0rwXNU","execution":{"iopub.status.busy":"2021-12-13T18:47:16.144543Z","iopub.execute_input":"2021-12-13T18:47:16.145192Z","iopub.status.idle":"2021-12-13T18:47:21.800449Z","shell.execute_reply.started":"2021-12-13T18:47:16.145153Z","shell.execute_reply":"2021-12-13T18:47:21.799355Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Using batch method converted individual characters to sequences of desired size\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n  print(repr(''.join(idx2char[item.numpy()])))","metadata":{"id":"Zr-JDdQm3Ewd","execution":{"iopub.status.busy":"2021-12-13T18:47:21.803092Z","iopub.execute_input":"2021-12-13T18:47:21.803448Z","iopub.status.idle":"2021-12-13T18:47:22.140731Z","shell.execute_reply.started":"2021-12-13T18:47:21.803409Z","shell.execute_reply":"2021-12-13T18:47:22.139802Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#### **Mapping function**","metadata":{"id":"62obB6BPEk-6"}},{"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","metadata":{"id":"2E1qhqdT3RN3","execution":{"iopub.status.busy":"2021-12-13T18:47:22.142109Z","iopub.execute_input":"2021-12-13T18:47:22.142624Z","iopub.status.idle":"2021-12-13T18:47:22.301802Z","shell.execute_reply.started":"2021-12-13T18:47:22.142581Z","shell.execute_reply":"2021-12-13T18:47:22.301042Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","metadata":{"id":"iSMOgVwJ3Wp2","execution":{"iopub.status.busy":"2021-12-13T18:47:22.303061Z","iopub.execute_input":"2021-12-13T18:47:22.303426Z","iopub.status.idle":"2021-12-13T18:47:22.605029Z","shell.execute_reply.started":"2021-12-13T18:47:22.303390Z","shell.execute_reply":"2021-12-13T18:47:22.604164Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### **Creating training batches for splitting text into manageable sequences**","metadata":{"id":"JLEdn7tJGcEm"}},{"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","metadata":{"id":"18mKD_EI3Z_k","execution":{"iopub.status.busy":"2021-12-13T18:47:22.609207Z","iopub.execute_input":"2021-12-13T18:47:22.611322Z","iopub.status.idle":"2021-12-13T18:47:22.631448Z","shell.execute_reply.started":"2021-12-13T18:47:22.611270Z","shell.execute_reply":"2021-12-13T18:47:22.630546Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units \nrnn_units = 1500 # keep between (1024 -> 1800) for best results ","metadata":{"id":"nmPdZBGG3lFb","execution":{"iopub.status.busy":"2021-12-13T18:47:22.635578Z","iopub.execute_input":"2021-12-13T18:47:22.637845Z","iopub.status.idle":"2021-12-13T18:47:22.644530Z","shell.execute_reply.started":"2021-12-13T18:47:22.637803Z","shell.execute_reply":"2021-12-13T18:47:22.643556Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### **Buliding the Model**\n#### **Four layers were used :**\n######  **1. Embedding layer :** The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions\n######  **2. GRU layer :** A type of RNN with size units=rnn_units (LSTM could also be used here.)\n###### **3. Dense layer :** The output layer, with vocab_size outputs and 'RELU' as the activation fuction \n###### **4. Dropout layer :** Benifits regularisation and prevents overfitting  \n","metadata":{"id":"Fgu__rqDHI9p"}},{"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n                               \n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n  \n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n\n    tf.keras.layers.Dense(vocab_size,activation='relu'),\n    \n    tf.keras.layers.Dropout(0.2),\n  ])\n  return model","metadata":{"id":"t64ZXPaH4AfR","execution":{"iopub.status.busy":"2021-12-13T18:47:22.649808Z","iopub.execute_input":"2021-12-13T18:47:22.652414Z","iopub.status.idle":"2021-12-13T18:47:22.662421Z","shell.execute_reply.started":"2021-12-13T18:47:22.652373Z","shell.execute_reply":"2021-12-13T18:47:22.661339Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"embedding_dim,BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2021-12-13T18:47:22.667389Z","iopub.execute_input":"2021-12-13T18:47:22.670423Z","iopub.status.idle":"2021-12-13T18:47:22.680729Z","shell.execute_reply.started":"2021-12-13T18:47:22.670378Z","shell.execute_reply":"2021-12-13T18:47:22.679692Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","metadata":{"id":"y7YNdwZA4HW6","execution":{"iopub.status.busy":"2021-12-13T18:47:22.685551Z","iopub.execute_input":"2021-12-13T18:47:22.686835Z","iopub.status.idle":"2021-12-13T18:47:23.329351Z","shell.execute_reply.started":"2021-12-13T18:47:22.686782Z","shell.execute_reply":"2021-12-13T18:47:23.328563Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","metadata":{"id":"1zMgA-w34Kcf","execution":{"iopub.status.busy":"2021-12-13T18:47:23.330849Z","iopub.execute_input":"2021-12-13T18:47:23.331165Z","iopub.status.idle":"2021-12-13T18:47:27.654380Z","shell.execute_reply.started":"2021-12-13T18:47:23.331131Z","shell.execute_reply":"2021-12-13T18:47:27.653574Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"#### **Model Summary**","metadata":{"id":"KPFvpo6xJrom"}},{"cell_type":"code","source":"model.summary()","metadata":{"id":"c_FQFXvb4Nin","execution":{"iopub.status.busy":"2021-12-13T18:47:27.657666Z","iopub.execute_input":"2021-12-13T18:47:27.658091Z","iopub.status.idle":"2021-12-13T18:47:27.666805Z","shell.execute_reply.started":"2021-12-13T18:47:27.658058Z","shell.execute_reply":"2021-12-13T18:47:27.665252Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","metadata":{"id":"jtJu9jF74Rbi","execution":{"iopub.status.busy":"2021-12-13T18:47:27.669368Z","iopub.execute_input":"2021-12-13T18:47:27.669919Z","iopub.status.idle":"2021-12-13T18:47:27.681615Z","shell.execute_reply.started":"2021-12-13T18:47:27.669875Z","shell.execute_reply":"2021-12-13T18:47:27.680045Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sampled_indices","metadata":{"id":"l21C3EHw4XPv","execution":{"iopub.status.busy":"2021-12-13T18:47:27.683965Z","iopub.execute_input":"2021-12-13T18:47:27.684459Z","iopub.status.idle":"2021-12-13T18:47:27.692909Z","shell.execute_reply.started":"2021-12-13T18:47:27.684413Z","shell.execute_reply":"2021-12-13T18:47:27.691636Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","metadata":{"id":"jYN-oiic4bXE","execution":{"iopub.status.busy":"2021-12-13T18:47:27.694871Z","iopub.execute_input":"2021-12-13T18:47:27.695450Z","iopub.status.idle":"2021-12-13T18:47:27.706756Z","shell.execute_reply.started":"2021-12-13T18:47:27.695404Z","shell.execute_reply":"2021-12-13T18:47:27.705662Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"#### **Attaching an optimizer, and a loss function**\n###### **tf.keras.losses.sparse_categorical_crossentropy** loss function works in this case because it is applied across the last dimension of the predictions.","metadata":{"id":"rcKIsigsJ1cE"}},{"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","metadata":{"id":"0TaUVaap4gI8","outputId":"69de9850-7375-435e-98fb-c6d658222bc8","execution":{"iopub.status.busy":"2021-12-13T18:47:27.708705Z","iopub.execute_input":"2021-12-13T18:47:27.709317Z","iopub.status.idle":"2021-12-13T18:47:27.727206Z","shell.execute_reply.started":"2021-12-13T18:47:27.709277Z","shell.execute_reply":"2021-12-13T18:47:27.726357Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#### **Adam optimiser gives the best result hands down**","metadata":{"id":"himU-fInKRYp"}},{"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)","metadata":{"id":"plUUV_mT4nUB","execution":{"iopub.status.busy":"2021-12-13T18:47:27.729363Z","iopub.execute_input":"2021-12-13T18:47:27.729975Z","iopub.status.idle":"2021-12-13T18:47:27.756484Z","shell.execute_reply.started":"2021-12-13T18:47:27.729935Z","shell.execute_reply":"2021-12-13T18:47:27.755708Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"#### **Configuring the checkpoints , ensuring that the checkpoints are saved during training**","metadata":{"id":"qqjrgQzOKcnU"}},{"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","metadata":{"id":"fXDJ7-rR4rTn","execution":{"iopub.status.busy":"2021-12-13T18:47:27.759081Z","iopub.execute_input":"2021-12-13T18:47:27.759653Z","iopub.status.idle":"2021-12-13T18:47:27.765404Z","shell.execute_reply.started":"2021-12-13T18:47:27.759610Z","shell.execute_reply":"2021-12-13T18:47:27.764507Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"#### **Rounds of training -> EPOCHS**","metadata":{"id":"an6jFLaGKvXJ"}},{"cell_type":"code","source":"# Configure it according to the loss you get at the end.\n# Ensure that the loss is between 1.4 to 1.1 for best meaningful text generation ensuring that it is always new lyrics/text , NOT same as in the training set\n# If the loss becomes less than 1 , then a lot of same text would be generated\n# Obviously we don't want the same , but something new\n\nEPOCHS=6","metadata":{"id":"3MT0yILo4ugb","execution":{"iopub.status.busy":"2021-12-13T18:47:27.769128Z","iopub.execute_input":"2021-12-13T18:47:27.769521Z","iopub.status.idle":"2021-12-13T18:47:27.776453Z","shell.execute_reply.started":"2021-12-13T18:47:27.769491Z","shell.execute_reply":"2021-12-13T18:47:27.775155Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","metadata":{"id":"JqBYt65U41vA","execution":{"iopub.status.busy":"2021-12-13T18:47:27.777893Z","iopub.execute_input":"2021-12-13T18:47:27.778385Z","iopub.status.idle":"2021-12-13T19:39:15.064157Z","shell.execute_reply.started":"2021-12-13T18:47:27.778343Z","shell.execute_reply":"2021-12-13T19:39:15.063306Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"#### **Generating text**","metadata":{"id":"k32Wp91lL9Q-"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","metadata":{"id":"UxZKKZhX5fsc","execution":{"iopub.status.busy":"2021-12-13T19:39:15.079646Z","iopub.execute_input":"2021-12-13T19:39:15.080209Z","iopub.status.idle":"2021-12-13T19:39:15.308558Z","shell.execute_reply.started":"2021-12-13T19:39:15.080169Z","shell.execute_reply":"2021-12-13T19:39:15.307802Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"5omajupS5ja6","execution":{"iopub.status.busy":"2021-12-13T19:39:15.309809Z","iopub.execute_input":"2021-12-13T19:39:15.310137Z","iopub.status.idle":"2021-12-13T19:39:15.319499Z","shell.execute_reply.started":"2021-12-13T19:39:15.310103Z","shell.execute_reply":"2021-12-13T19:39:15.316522Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"#### **Prediction Loop : the code block that generates the text**","metadata":{"id":"vQ6O9y6HMLjI"}},{"cell_type":"code","source":"def generate_text(model, chars_to_generate , temp , start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = chars_to_generate\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = temp\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions / temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","metadata":{"id":"g73reOa05mcN","execution":{"iopub.status.busy":"2021-12-13T19:39:15.320975Z","iopub.execute_input":"2021-12-13T19:39:15.321700Z","iopub.status.idle":"2021-12-13T19:39:15.331517Z","shell.execute_reply.started":"2021-12-13T19:39:15.321551Z","shell.execute_reply":"2021-12-13T19:39:15.330610Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"#### **Generation (finally :))**","metadata":{"id":"5vSQLUizOGGe"}},{"cell_type":"code","source":"from numpy import arange\n\n# Printing the generated text\n# Temperature 1.0 gives the craziest output and 0.1 gives the lowest varience\n# Keeping the temperature 0.35 gives best meaningful / coherent text.\n\n# Give the seed string as the first word of generate text\n# for i in range(500):\n#     x.append(generate_text(model , chars_to_generate , 0.35 , start_string=u\"baby \"))\n#     x.append(\"||\")\ncounts = 1000\nbaby = [generate_text(model , chars_to_generate , 0.35 , start_string=u\"baby \") for _ in range(counts)]\nprint(\"baby\")\nlove = [generate_text(model , chars_to_generate , 0.35 , start_string=u\"love \") for _ in range(counts)]\nprint(\"love\")\ni_word = [generate_text(model , chars_to_generate , 0.35 , start_string=u\"i \") for _ in range(counts)]\nprint(\"i\")\nyour = [generate_text(model , chars_to_generate , 0.35 , start_string=u\"your \") for _ in range(counts)]\nprint(\"your\")\nall_ = [generate_text(model , chars_to_generate , 0.35 , start_string=u\"all \") for _ in range(counts)]\nprint(\"all\")\nmy = [generate_text(model , chars_to_generate , 0.35 , start_string=u\"my \") for _ in range(counts)]\nprint(\"my\")\n\n# Uncomment below to check the variences ==>\n\n# for i in arange(0.1,1.1,0.05):\n#   print(\"==============\")\n#   print(\"FOR TEMP : {} \".format(i))\n#   print(\"==============\")\n#   print(generate_text(model , chars_to_generate , i , start_string=u\"love \"))\n#   print()\n","metadata":{"id":"QHOx_XJb5yQr","execution":{"iopub.status.busy":"2021-12-14T01:20:54.150787Z","iopub.execute_input":"2021-12-14T01:20:54.151095Z","iopub.status.idle":"2021-12-14T03:14:50.937709Z","shell.execute_reply.started":"2021-12-14T01:20:54.151065Z","shell.execute_reply":"2021-12-14T03:14:50.936745Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"my = \"||\".join(my)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T03:14:50.940623Z","iopub.execute_input":"2021-12-14T03:14:50.940905Z","iopub.status.idle":"2021-12-14T03:14:50.946537Z","shell.execute_reply.started":"2021-12-14T03:14:50.940877Z","shell.execute_reply":"2021-12-14T03:14:50.945620Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"with open(\"my.txt\", \"w\") as text_file:\n    text_file.write(my)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T03:14:50.948122Z","iopub.execute_input":"2021-12-14T03:14:50.948892Z","iopub.status.idle":"2021-12-14T03:14:50.961213Z","shell.execute_reply.started":"2021-12-14T03:14:50.948849Z","shell.execute_reply":"2021-12-14T03:14:50.960312Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}